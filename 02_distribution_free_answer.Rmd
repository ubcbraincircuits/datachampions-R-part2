---
title: "Wilcoxon and permutation test"
output: html_document
date: '2022-06-23'
---
# Load the data
The data below come from Hogg & Tanis, example 8.4-6. It involves the weights of packaging from two companies selling the same product. We have 8 observations from each company, A and B. We would like to know if the distribution of weights shifted between the companies. A quick boxplot reveals the data have similar spread but may be skew and non-normal. With such a small sample it might be dangerous to assume normality.

Website: https://data.library.virginia.edu/the-wilcoxon-rank-sum-test/#:~:text=The%20impact%20of%20ties%20means,p%2Dvalue%20with%20ties%E2%80%9D.

```{r}
A <- c(117.1, 121.3, 127.8, 121.9, 117.4, 124.5, 119.5, 115.1)
B <- c(123.5, 125.3, 126.5, 127.9, 122.1, 125.6, 129.8, 117.2)
dat <- data.frame(weight = c(A,B), 
                  company = rep(c("A","B"), each=8))
dat$company <- factor(dat$company)
boxplot(weight ~ company, data = dat)
```

# Perform the Wilcoxon test
Now we run the Wilcoxon Rank Sum Test using the wilcox.test function. Again, the null is that the distributions have the same mean. The alternative is two-sided. We have no idea if one distribution is shifted to the left or right of the other.

```{r}
# Use help(wilcox.test) to determine the inputs for this function. You can use ~ to specify the relationship between weight and company.
wt = wilcox.test(weight ~ company, data = dat, exact = TRUE)
```

First we notice the p-value is very close to 0.05, but still smaller. Based on this result, we may reject the null hypothesis. The alternative hypothesis is stated as the “true location shift is not equal to 0”. That’s another way of saying “the distribution of one population is shifted to the left or right of the other,” which implies different means.

The Wilcoxon statistic is returned as W = 13. This is NOT an estimate of the difference in means. This is actually the number of times that a package weight from company B is less than a package weight from company A.

Similarly to the t-test, we can plot the distribution of the W statistic. We can then identify the rejection regions as well as the observed test statistic value. 

# Wilcoxon distribution
```{r}
x <- -1: (8*8 + 1) 
# The p-value is calculated by determining the index for which the cumulative sum of fx exceeds 0.05. Therefore, we should be careful to index correctly on x as well.

fx <- dwilcox(x, length(A), length(B)) # dwilcox is a builtin R function that determines the Wilcoxon distribution given the sizes of the two samples

thresholdValue <- 0.025 # two-sided test since we want 0.025 as both ends

ix <- which(cumsum(fx) <= thresholdValue) # cumulative sum is summing probabilities up to the edge of the rejection region (e.g. thresholdValue) in order to determine the rejection region. However, the edge of the rejection region might not be exactly at a significance level of 0.05 as the value of alpha will likely fall between the discrete values of the W distribution.

tx <- x[length(ix)] # determines the maximum x-value for which the sum of probabilities is still under the threshold value

# Plot the distribution, rejection region, and observed test statistic
plot(x, fx, type = "h", col = "violet",
     main =  "Probabilities (density) of Wilcoxon-Statist.(n=8, m=8)") + abline(v=wt$statistic, col="blue")+ polygon(c(x[x <= tx], tx, tx), c(fx[x <= tx], 0, 0), col=rgb(1, 0, 0,0.5), border=NA) 
```
As shown in the example in the powerpoint slides, the wilcoxon distribution is discrete. This means we can calculate exact p-values or use a normal approximation.

By default, the wilcox.test function will calculate exact p-values if the samples contains less than 50 finite values and there are no ties in the values. Otherwise a normal approximation is used. To force the normal approximation, set exact = FALSE.

When we use the normal approximation the phrase “with continuity correction” is added to the name of the test. A continuity correction is an adjustment that is made when a discrete distribution is approximated by a continuous distribution. The normal approximation is very good and computationally faster for samples larger than 50.

Now, we'll go back to the slides.

# Permutation testing

Website: https://thomasleeper.com/Rcourse/Tutorials/permutationtests.html 
Note: this website gives a pretty good explanation on permutation tests but the code may not be super accurate as it only resamples from one group

An increasingly common statistical tool for constructing sampling distributions is the permutation test (or sometimes called a randomization test). Like bootstrapping, a permutation test estimates, rather than calculates based on theoretical assumptions" - sampling distribution (called the “permutation distribution”) by resampling the observed data. Specifically, we can “shuffle” or permute the observed data (e.g., by assigning different outcome values to each observation from among the set of actually observed outcomes).

When we permute the outcome values during the test, we therefore see all of the possible alternative treatment assignments we could have had and where the mean-difference in our observed data falls relative to all of the differences we could have seen if the outcome was independent of treatment assignment. While a permutation test requires that we see all possible permutations of the data (which can become quite large), we can easily conduct “approximate permutation tests” by simply conducting a vary large number of resamples. That process should, in expectation, approximate the permutation distribution.

The same two groups of data can be investigated further using permutation testing. We can determine whether the means of the two groups are statistically significant by combining the two and drawing random samples to put into two new groups. We then find the difference between the means of these two new groups and plot it on a distribution. After doing this for numerous iterations, we arrive at our permutation distribution. 

As done above, we can plot the distribution of the permutation statistic. We can then identify the rejection regions as well as the observed test statistic value. 

```{r}
# Combine the data from group A and B 
pooled <- c(A,B)

# Try resampling the combined data from above. What are we trying to calculate here? Use help(sample) to get started.
mean(sample(pooled, length(A), FALSE))-mean(sample(pooled, length(B), FALSE))

```

Now that we've done one resample, we can do this over and over again as many times as needed using the replicate function. You can use help(replicate) to see what the function does. 

```{r}
# Divide the combined data into two groups and calculate their difference in means 2000 times  
nresamples <- 2000
dist <- replicate(nresamples, mean(sample(pooled, length(A), FALSE))-mean(sample(pooled, length(B), FALSE)))

# Plot the histogram
histogram = hist(dist, xlim = c(-6, 6), col = "black", breaks = 100)
thresholdValue <- 0.025
ix <- which(cumsum(histogram$counts)/nresamples <= thresholdValue)
tx <- histogram$breaks[length(ix)]
abline(v = mean(A)-mean(B), col = "blue", lwd = 2)
polygon(x=c(-6,-6,tx,tx), y=c(0,60,60,0), col="#FF990022", border=F)
```

First we notice the original difference in means is inside the rejection region. Based on this result, we may reject the null hypothesis.  Our null hypothesis is that the two treatment groups do not differ on the outcome (i.e., that the outcome is observed independently of treatment assignment). That’s another way of saying that the two groups do differ in outcome which implies different means.
